{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import operator\n",
    "import spacy\n",
    "\n",
    "DATA_PATH = \"data/\"\n",
    "FILE_NAME = 'spacy_entity_features.csv'\n",
    "\n",
    "df_train = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "df_test  = pd.read_csv(DATA_PATH + 'test.csv')\n",
    "# df_train = df_train.head(10)\n",
    "# df_test  = df_test.head(10)\n",
    "\n",
    "ENTITY_TYPES = [\n",
    "    'PERSON', 'NORP', 'FACILITY', 'ORG',\n",
    "    'GPE', 'LOC', 'PRODUCT', 'EVENT',\n",
    "    'WORK_OF_ART', 'LANGUAGE', 'DATE', 'TIME',\n",
    "    'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL',\n",
    "    'CARDINAL'\n",
    "]\n",
    "\n",
    "ENTITY_FEATURES = ['Q1_' + i for i in ENTITY_TYPES] +\\\n",
    "               ['Q2_' + i for i in ENTITY_TYPES] +\\\n",
    "               ['SHARED_' + i for i in ENTITY_TYPES] +\\\n",
    "               ['JACCARD_' + i for i in ENTITY_TYPES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __jaccard_distance(A, B, I):\n",
    "    if A == B == 0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 1.0 - ( I / ((A + B) - I))\n",
    "\n",
    "def get_entity_counts(row):\n",
    "    doc1 = [] if type(row['question1']) != str else nlp(unicode(row['question1']))\n",
    "    doc2 = [] if type(row['question2']) != str else nlp(unicode(row['question2']))\n",
    "    ent_data = {ent: [set([]), set([])] for ent in ENTITY_TYPES}\n",
    "    ent_features = {}\n",
    "\n",
    "    if doc1 == doc2 == []:\n",
    "        return '0:0:0:2'\n",
    "\n",
    "    for t in doc1:\n",
    "        if str(t.ent_type_) in ent_data:\n",
    "            ent_data[str(t.ent_type_)][0].add(t.text.lower())\n",
    "\n",
    "    for t in doc2:\n",
    "        if str(t.ent_type_) in ent_data:\n",
    "            ent_data[str(t.ent_type_)][1].add(t.text.lower())\n",
    "\n",
    "    for ent in ENTITY_TYPES:\n",
    "        q1 = len(ent_data[ent][0])\n",
    "        q2 = len(ent_data[ent][1])\n",
    "        shared = len(ent_data[ent][0].intersection(ent_data[ent][1]))\n",
    "        ent_features['Q1_' + ent] = q1\n",
    "        ent_features['Q2_' + ent] = q2\n",
    "        ent_features['SHARED_' + ent] = shared\n",
    "        ent_features['JACCARD_' + ent] = __jaccard_distance(q1, q2, shared)\n",
    "\n",
    "    return ('{}:{}:{}:{}:'*len(ENTITY_TYPES)).format(*[ent_features[i] for i in ENTITY_FEATURES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    nlp = spacy.load('en')\n",
    "\n",
    "    df = pd.concat([df_train, df_test])\n",
    "    df['word_shares'] = df.apply(get_entity_counts, axis=1, raw=True)\n",
    "\n",
    "    x = pd.DataFrame()\n",
    "    for index, ent_features in enumerate(ENTITY_FEATURES):\n",
    "        x[ent_features] = df['word_shares'].apply(lambda x: float(x.split(':')[ index ]))\n",
    "\n",
    "    x_train = x[:df_train.shape[0]]\n",
    "    x_test  = x[df_train.shape[0]:]\n",
    "    x_train.to_csv(DATA_PATH + 'train_' + FILE_NAME)\n",
    "    x_test.to_csv(DATA_PATH + 'test_' + FILE_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
