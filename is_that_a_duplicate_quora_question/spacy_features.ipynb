{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import operator\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"../../../data/quora_data_set/\"\n",
    "FILE_NAME = 'spacy_features.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "POS_TYPES = [\n",
    "    'PUNCT',\n",
    "    'SYM',\n",
    "    'ADJ',\n",
    "    'VERB',\n",
    "    'CONJ',\n",
    "    'NUM',\n",
    "    'DET',\n",
    "    'ADV',\n",
    "    'ADP',\n",
    "    'NOUN',\n",
    "    'PROPN',\n",
    "    'PART',\n",
    "    'PRON',\n",
    "    'INTJ',\n",
    "]\n",
    "\n",
    "POS_FEATURES = ['Q1_' + i for i in POS_TYPES] +\\\n",
    "               ['Q2_' + i for i in POS_TYPES] +\\\n",
    "               ['SHARED_' + i for i in POS_TYPES] +\\\n",
    "               ['JACCARD_' + i for i in POS_TYPES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ENTITY_TYPES = [\n",
    "    'PERSON',\n",
    "    'NORP',\n",
    "    'FACILITY',\n",
    "    'ORG',\n",
    "    'GPE',\n",
    "    'LOC',\n",
    "    'PRODUCT',\n",
    "    'EVENT',\n",
    "    'WORK_OF_ART',\n",
    "    'LANGUAGE',\n",
    "    'DATE',\n",
    "    'TIME',\n",
    "    'PERCENT',\n",
    "    'MONEY',\n",
    "    'QUANTITY',\n",
    "    'ORDINAL',\n",
    "    'CARDINAL'\n",
    "]\n",
    "\n",
    "ENTITY_FEATURES = ['Q1_' + i for i in ENTITY_TYPES] +\\\n",
    "               ['Q2_' + i for i in ENTITY_TYPES] +\\\n",
    "               ['SHARED_' + i for i in ENTITY_TYPES] +\\\n",
    "               ['JACCARD_' + i for i in ENTITY_TYPES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __jaccard_distance(A, B, I):\n",
    "    if A == B == 0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 1.0 - ( I / ((A + B) - I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pos_counts(row):\n",
    "\n",
    "        doc1 = [] if type(row['question1']) != str else nlp(row['question1'])\n",
    "        doc2 = [] if type(row['question2']) != str else nlp(row['question2'])\n",
    "        pos_data = {pos: [set([]), set([])] for pos in POS_TYPES}\n",
    "        pos_features = {}\n",
    "\n",
    "        if doc1 == doc2 == []:\n",
    "            return '0:0:0:2'\n",
    "\n",
    "        for t in doc1:\n",
    "            if t.pos_ in pos_data:\n",
    "                pos_data[t.pos_][0].add(t.text.lower())\n",
    "\n",
    "        for t in doc2:\n",
    "            if t.pos_ in pos_data:\n",
    "                pos_data[t.pos_][1].add(t.text.lower())\n",
    "\n",
    "        for pos in POS_TYPES:\n",
    "            q1 = len(pos_data[pos][0])\n",
    "            q2 = len(pos_data[pos][1])\n",
    "            shared = len(pos_data[pos][0].intersection(pos_data[pos][1]))\n",
    "            pos_features['Q1_' + pos] = q1\n",
    "            pos_features['Q2_' + pos] = q2\n",
    "            pos_features['SHARED_' + pos] = shared\n",
    "            pos_features['JACCARD_' + pos] = __jaccard_distance(q1, q2, shared)\n",
    "\n",
    "        return ('{}:{}:{}:{}:'*len(POS_TYPES)).format(*[pos_features[i] for i in POS_FEATURES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_entity_counts(row):\n",
    "\n",
    "        doc1 = [] if type(row['question1']) != str else nlp(row['question1'])\n",
    "        doc2 = [] if type(row['question2']) != str else nlp(row['question2'])\n",
    "        ent_data = {ent: [set([]), set([])] for ent in ENTITY_TYPES}\n",
    "        ent_features = {}\n",
    "\n",
    "        if doc1 == doc2 == []:\n",
    "            return '0:0:0:2'\n",
    "\n",
    "        for t in doc1:\n",
    "            if str(t.ent_type_) in ent_data:\n",
    "                ent_data[str(t.ent_type_)][0].add(t.text.lower())\n",
    "\n",
    "        for t in doc2:\n",
    "            if str(t.ent_type_) in ent_data:\n",
    "                ent_data[str(t.ent_type_)][2].add(t.text.lower())\n",
    "\n",
    "        for ent in ENTITY_TYPES:\n",
    "            q1 = len(ent_data[ent][0])\n",
    "            q2 = len(ent_data[ent][1])\n",
    "            shared = len(ent_data[ent][0].intersection(ent_data[ent][1]))\n",
    "            ent_features['Q1_' + ent] = q1\n",
    "            ent_features['Q2_' + ent] = q2\n",
    "            ent_features['SHARED_' + ent] = shared\n",
    "            ent_features['JACCARD_' + ent] = __jaccard_distance(q1, q2, shared)\n",
    "\n",
    "        return ('{}:{}:{}:{}:'*len(ENTITY_TYPES)).format(*[ent_features[i] for i in ENTITY_FEATURES])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[93m    Warning: no model found for 'en'\u001b[0m\n",
      "\n",
      "    Only loading the 'en' tokenizer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "df_test  = pd.read_csv(DATA_PATH + 'test.csv')\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_train = df_train.head()\n",
    "sample_test = df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "#     df = pd.concat([df_train, df_test])\n",
    "    df = pd.concat([sample_train, sample_test])\n",
    "    df['word_shares'] = df.apply(get_entity_counts, axis=1, raw=True)\n",
    "\n",
    "    x = pd.DataFrame()\n",
    "\n",
    "    for index, ent_features in enumerate(ENTITY_FEATURES):\n",
    "        x[ent_features] = df['word_shares'].apply(lambda x: float(x.split(':')[ index ]))\n",
    "\n",
    "#     x_train = x[:df_train.shape[0]]\n",
    "#     x_test  = x[df_train.shape[0]:]\n",
    "    x_train = x[:sample_train.shape[0]]\n",
    "    x_test  = x[sample_test.shape[0]:]\n",
    "    x_train.to_csv(DATA_PATH + 'train_' + FILE_NAME)\n",
    "    x_test.to_csv(DATA_PATH + 'test_' + FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
